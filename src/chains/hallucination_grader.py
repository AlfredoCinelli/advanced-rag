"""
Module defining a Grader Chain to assess hallucination in the answer compared to the context.

More in detail the chain assess if the answer generated by the LLM
is grounded in the retrieved documents (i.e., provided context) or not.
"""

# Import packages and modules

from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel, Field
from langchain_core.runnables import RunnableSequence, RunnableLambda
from langchain_ollama import ChatOllama
import warnings
from src.utils.misc import format_docs
from src.constants import HALLUCINATION_GRADER_TEMPLATE

warnings.filterwarnings("ignore")
load_dotenv("local/.env")

# Define LLM
llm = ChatOllama(
    model="mistral-nemo", #Â qwen2.5
    temperature=0.0,
)


# Define Grader Structure
class GradeHallucination(BaseModel):
    """Binary score for hallucination present in the generated answer."""

    binary_score: str = Field(
        ...,
        description="Passage is grounded in the documents, 'yes' or 'no'.",
    )


structured_llm_answ_grader = llm.with_structured_output(
    schema=GradeHallucination,
    method="json_schema",
)

# Assemble prompt
hallucination_prompt = PromptTemplate(
    template=HALLUCINATION_GRADER_TEMPLATE,
    input_variables=["documents", "generation"],
)

# Assemble chain
hallucination_grader: RunnableSequence = (
    RunnableLambda(
        lambda x: {
            "documents": format_docs(x["documents"]),
            "generation": x["generation"],
        }
    )
    | hallucination_prompt
    | structured_llm_answ_grader
)
