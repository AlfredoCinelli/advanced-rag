"""
Module defining a Grader Chain to assess hallucination in the answer compared to the original question.

More in detail the chain assess if the answer generated by the LLM
is grounded in the user query or not.
"""

# Import packages and modules

from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel, Field
from langchain_core.runnables import RunnableSequence
from langchain_ollama import ChatOllama
import warnings
from src.constants import ANSWER_GRADER_TEMPLATE

warnings.filterwarnings("ignore")
load_dotenv("local/.env")

# Define LLM
llm = ChatOllama(
    model="mistral-nemo", # qwen2.5
    temperature=0.0,
)

# Define Grader Structure
class GradeAnswer(BaseModel):
    """Binary score for hallucination present in the generated answer."""

    binary_score: str = Field(
        ...,
        description="Passage addresses the question, 'yes' or 'no'.",
    )


structured_llm_answ_grader = llm.with_structured_output(
    schema=GradeAnswer,
    method="json_schema",
)

# Assemble prompt
answer_prompt = PromptTemplate(
    template=ANSWER_GRADER_TEMPLATE,
    input_variables=["question", "generation"],
)

# Assemble chain
answer_grader: RunnableSequence = answer_prompt | structured_llm_answ_grader
